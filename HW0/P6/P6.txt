# Your answers here
1) There is no consistency between when the jobs finish (i.e., in a given run, job 2 might finish before job 3, and in another run, job 3 might finish before job 2). Furthermore, there is no guarantee that jobs started first finish first. Last, sometimes jobs will be added and completed before other (previously running jobs) finish. 

This is important because one can imagine parallel situations where the result of one computation is necessary for another computation. For example, in computational science, and more specifically finite difference solutions to partial differential equations, we can discretize the space and solve the equations on different domains separately. However, at the edges, we need information from adjacent domains for the calculation of the derivative. The inconsistencies described above could lead to unexpected slowdowns if one processor is waiting for the results of another processor when the programmer does not realize that such waiting would happen.

2) The parallel program is slower for very fast wait times, and faster for slower wait times. This intutively makes sense because there is some communication delay between processors. If the time for communication is longer than the time to complete jobs and go on to the next in serial, the parallel process will actually take longer. This is exactly what we see in the earlier part of the graph attached.
