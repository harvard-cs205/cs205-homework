Part 1:
========================
Finished after 911 iterations, 214.33168 ms total, 0.235270779363 ms per iteration
Found 2 regions

Finished after 531 iterations, 124.05312 ms total, 0.233621694915 ms per iteration
Found 35 regions

Part 2:
========================
Finished after 529 iterations, 133.35928 ms total, 0.252096937618 ms per iteration
Found 2 regions

Finished after 269 iterations, 68.04856 ms total, 0.252968624535 ms per iteration
Found 35 regions

Part 3:
========================
Finished after 8 iterations, 2.59184 ms total, 0.32398 ms per iteration
Found 2 regions

Finished after 8 iterations, 2.4332 ms total, 0.30415 ms per iteration
Found 35 regions

Part 4:
========================
Finished after 10 iterations, 7.37784 ms total, 0.737784 ms per iteration
Found 2 regions

Finished after 9 iterations, 6.66816 ms total, 0.740906666667 ms per iteration
Found 35 regions

Using a single-thread for caching seems to have made the running time much slower (looking at the time per iteration
.) Computation is serialized during this caching process, so there is a big upfront cost that needs to be weighed
against the ongoing cost of expensive, potentially serialized, main memory accesses.

I suspect the worst case memory access scenario where neighbouring nodes all try to access main memory for a single
cached value only happens quite late in the process. Earlier iterations are probably facing more diversity of memory
accesses since there's a greater range of values still remaining. It follows then, that perhaps with much more
complex mazes, there could be scenarios where the overall iteration count will be high, but parts of the maze will
have already "stabilized" early in the process. Therefore, these "stabilized" parts are repeatedly drawing on the same
cached values. It seems there aren't enough iterations in these examples for it to be worth the upfront cost (at
least on my hardware setup.) Potentially, other setups that have different computation and memory access speeds might
reveal differences in the results since the trade-off is weighed differently.

Part 5:
========================
If instead of atomic_min() we use min(), then the update step is not done in a single transaction, meaning between
the min() check and the subsequent update, a different thread could update the reference value. Suppose this
different thread actually updated the reference to an even lower value than what we had intended. Now, if we go ahead
 with our update, we are actually *increasing* the reference value. It could therefore also lead to an increase in
 this value between iterations.

 So while it would be faster to do without atomic_min() as memory access is not serialized, it is also slow since it
 means more iterations have to be performed. While empirical testing is needed to determine which is the better
 trade-off, my sense is that for simple, low-iteration mazes, atomic_min() is going to present a significant overhead
  so it might be better to do a few more iterations. The opposite is likely true for high-iteration mazes.

This makes things more inefficient since this cache is potentially being updated with worse values. However, because
ultimately the stopping condition is whether or not any more updates have been performed, it won't effect the
correctness of the algorithm.