Part 1
------
Maze 1:
2 regions (This is the same for all implementations.)
Finished after 915 iterations, 36.31 ms total, 0.03968 ms per iteration

Maze 2:
35 regions (This is the same for all implementations.)
Finished after 532 iterations, 20.12 ms total, 0.03783 ms per iteration

Part 2
------
Maze 1:
Finished after 529 iterations, 20.75 ms total, 0.03923 ms per iteration

Maze 2:
Finished after 276 iterations, 10.73 ms total, 0.03887 ms per iteration

Part 3
------
Maze 1:
Finished after 9 iterations, 0.46 ms total, 0.05115 ms per iteration

Maze 2:
Finished after 9 iterations, 0.46 ms total, 0.05176 ms per iteration

Part 4:
-------
Maze 1:
Finished after 9 iterations, 1.00 ms total, 0.1111 ms per iteration

Maze 2:
Finished after 8 iterations, 0.89 ms total, 0.1147 ms per iteration

Adding efficient grandparents doubled both total computation and average kernel execution time, while removing only a single iteration for one of the mazes.  The increase in time is due to adding a lot of serialized computation (double for loops over the entire work group) within a single thread in the work group.  This single thread ends up doing several global memory accesses, which are slow relative to the amount of computation within each thread.  There are fewer global memory accesses than are done in total in the parallelized version, but many of them still occur in the single thread and the end result is that the process is slower.  

As for speculating on how this might perform differently on a different GPU, for this specific maze, efficient grandparents only removes a single iteration, so global memory reads would have to be extremely fast for this single reduction in iterations to outweigh the increased number of memory reads.  On a different maze (one that required more iterations to complete), I might expect the efficient grandparents implementation to be faster overall on the GPU that I am using if it reduced iterations significantly, even though the individual iterations would still be slower.  

Part 5:
-------
The atomic_min() function finds the minimum of the two arguments and writes the result back into the memory location of the part of labels[] that needs to be updated.  The problem with using min() instead of atomic_min() a thread can rewrite a memory location in between another thread computing the minimum writing to the memory location.    While this shouldn't cause errors, it could result in a value of labels increasing.  While min() might be faster than atomic_min() (and therefore make each thread faster), if it allows labels to increase, it could result in increased iterations, which might make the total compute time longer.  



