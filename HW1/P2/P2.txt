# Your discussion here

The original implementation of the code does not balance work well. We see that the total number
of iterations per partition varies over 3 orders of magnitude (from 10^3 to 10^7)!

This mainly occurs because a large portion of the jobs complete in a relatively short period of time
(roughly 10^0 -> 10^3 iterations) while a non-negligible amount complete in an extremely long time of roughly
10^6 iterations. If one partition is given more long jobs than others, it will ultimately do more work.

To fix this load-balancing problem, I took two approaches.

1) See P2b.py, P2b_hist.png, and P2b_mask_of_expensive.png. Here, I split jobs into two types: expensive jobs
and non-expensive jobs. I divied the expensive and non-expensive jobs equally between each partition. We see that
the work done in each partition collapses to the same order of magnitude, roughly 10^6 iterations, as desired.
Although this strategy appears quite lucrative, there is definitely a tradeoff: we have to do much more preprocessing of
the data. The most significant cost of pre-processing the data comes from creating an index for each job (zipByIndex)
and then sorting the jobs by key so that they are passed to the correct partition (this constitutes a shuffle). For large
datasets, this shuffle could be prohibitively expensive. For our dataset, although the load is balanced better,
P2b.py does not run much faster than the original work.

2) See P2b_alternative.py, P2b_alternative_hist.png. Here we randomly repartition the jobs to each partition using the
"repartition" command. The hope here is that expensive jobs will be evenly spread between partitions. Without the
repartition command, due to the periodic nature of cartesian, certain partitions will be given an unequal number of
expensive jobs. The load is better balanced than the original algorithm, but the amount of work on each partition
still varies over 2 orders of magnitude. In order to repartition the data, we had to suffer through another shuffle as
well. It is not clear if 1) or 2) is the better algorithm, but 1) definitely distributes the load more evenly at the
cost of increased complexity and preprocessing.