# Your discussion here

In P2a, we did a naive partitioning of the ordered pairs and performed mapped it with calculations from the mandelbrot function. Even in parallel, it took about 53 seconds for this to run. It appears that about 90% of the workers did very few calculations, while the other 10% did much more. We had workers that hardly took 0.3 seconds to run, and workers that took nearly 20 seconds to run. We can see from the histogram that most of the partitions had very few iterations run from the mandelbrot function. It is clear that this naive partitioning strategy doesn't split the work equally among all the workers.

In P2b, the partitioning strategy that I used was to randomize the partition that each ordered pair gets placed in. Overall, thi stook 50 seconds to run. This gave us more of a normal distribution of iterations run by the partitions, in which every worker took roughly 1-2 seconds to run. In other words, most of the partitions ran about the same number of calculations. This is because, originally, the ordered pairs that caused a greater number of iterations to run in the mandelbrot function were concentrated in a few of the 100 partitions we had set. By randomly placing which partition each ordered pair goes to, we distribute the work more evenly. Unfortunately, the partiitonBy operation caused 41 seconds of shuffling, which is problematic and becomes the bottleneck of our script.