# Your discussion here

In P2a's histogram, we can see that pyspark's default partitioning scheme does not efficiently divide the computations amongst the workers. There is a strong density of partitions for the easier computations while the "stragglers" have very few partitions. This is causing the program to stall in order for the harder computations to complete.

I choose to use pyspark's "repartition" function in order to see how it would compare to the default partitioning scheme. As shown by the P2b histogram, the partitions are more evenly spread amongst the workers. This is because repartition uses a shuffle to repartition the randomly redistribute the data. The repartition helps for this problem but we need to be wary of using it (as I found out later in the homework) as it performs costly shuffles. I also decided to permute the pixels before passing them to the mandelbrot function to ensure that the partitions have a uniform spread of easier/more difficult computations to perform. This is another step to ensure that high intensity pixels are distributed randomly across partitions and therefore enforcing workers to complete their tasks around the same time. This problem shows how pyspark uses partitioned RDDs to distribute work.