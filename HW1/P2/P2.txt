# Your discussion here
From the graph in P2a, it is clear that the number of mandelbrot iterations varies distinctly based on its coordinates. To balance the amount of work across all the partitions, my strategy is to randomly shuffle the list of input coordinates. With such a large number of data points (4000000) randomly sorted into a small number of partitions (100), it is likely that each partition has a similar percentage of coordinates with high versus low iterations. From the graph in P2b, the work is balanced really well, with total iterations in the range of [1960000, 2100000], a spread of only 250000. In contrast, the total iterations in P2a_hist covers the a wider range of [0,9000000]

In terms of costs, as each coordinate's mandelbrot calculations is independent of the other coordinates, randomly distributing the coordinates across the partitions will have little effect in computational time of the mandelbrot values. However, if the drawing of the diagram requires the coordinates to be fed into matplotlib in a sequential order, then a sorting cost will be incurred.
