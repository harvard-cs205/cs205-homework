# Your discussion here

The initial mandelbrot calculation without partitioning had a few workers doing a lot of work, and about 85 workers doing little to no work. The standard deviation of the number of calculations over the 100 workers was 5152754.37964. Looking at the Spark job page, you can see the individual runtimes of each worker, most workers ran in .1 or .1 seconds, but some ran for as long as 7 seconds.

My initial test was just to randomize the distribution of the nodes. The shuffle took 22 seconds, but now each worker was working .7-.8 seconds and the std went down to 31141.589793. Although in this example the total calculation time was roughly the same because of the 22 second shuffle cost, as the number of nodes scales it will pay off much better.

My next approach was to equally split the middle of the mandelbrot plot where there is the heaviest computation, then randomly partition the rest of the graph. I took the box x = [800, 1000], y = [700, 1200] and subsequently the std deviation was 24852.3251531. The shuffle was 24 seconds and the workers were again about .8 seconds of compute time. This is what's shown in the histogram plot for 2b.
