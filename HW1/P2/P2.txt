When determining different options for distributing the workload of computing the Mandelbrot set across the 100 partitions I considered a few different strategies. The first strategy that I considered was to segment the 2000x2000 grid where I would evenly distribute the areas of high iterations across all partitions and then distribute the areas of low iterations. I realized that my partitioning function would need to have very specific conditions requiring specific information about the data. Because I was trying to develop a general method that will scale to large data sets, that I have little knowledge about, I quickly abandoned this approach. My second approach was abandoned similarly. I began by defining the grid with a single array: range(0,2000), then randomly shuffled it before taking a cartesian product to get the 2000x2000 grid. My approach hinged creating unordered pairs of pixels. I abandoned this method for the same reason as before, I won’t generally have this knowledge of the data I’m processing.

My final strategy incorporates the rdd.repartition(N) command, where N is the number of partitions desired. This command randomly reassigns the data in the original partitions to new partitions in an attempt to balance work across the N partitions. This approach yielded sufficient results in balancing the number of iterations per partition as can be seen in the comparison between P2a_hist and P2b_hist. This shuffle, while costly initially, sufficiently cuts down on the total cost of my script due to the balanced work across partitions, cutting down on stragglers and hanging tasks.