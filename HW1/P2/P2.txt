# Your discussion here


Ultimately, we get different results from the two of these. Naturally, having another partitionBy will lead to another shuffle, and this does, on my local machine, lead to a slight slowdown. However, this is not problematic, since a computation like this can be very easily parallelized (since it is embarassingly parallel), and therefore being able to balance loads will allow this to work extremely well on a distributed cluster, such as on AWS.

Looking at the graphs directly, we see that in the first part, most of the partitions have very little load, and a few of them have a ton of load (note the log scale). 

However, in the second technique, we find that all of them have very close to the same load, and that load is much lower than the max load from the first part! This is very beneficial in a highly parallel problem like this one.

As the problem stated, there are many partitioning strategies. I chose not to employ a more random partitioning, since I had information about this problem's setup (such as how big the final plot woudl be) that I could exploit to get a very good partitioning. Thus, if you see the final histogram that came from this data, you will see that the load is extremely well balanaced - the load variance is very low. Thus, even though this introduces another shuffle (although it should not be a worse shuffle than a random partitioning approach), it leads to very good load balancing, and thus likely to a faster solution on a highly parallelized compute configuration.