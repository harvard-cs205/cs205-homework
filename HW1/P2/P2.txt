# Your discussion here
1) The partitioning chosen was to uniformly randomly distribute the pixels before the mandelbrot computation. This choice was made based on the first histogram with the default partitioning. In the first histogram, the majority (over 85%) of the pixels had very few iterations. Thus, uniformly mixing the few high iteration pixels along with the majority low iteration pixels intuitively made sense as a more equitable partitioning strategy. The second histogram, while not being uniform in frequency across pixel bins is still much better distributed than the original histogram (it appears to resemble a normal distribution).
2) Using partitionBy seems to be expensive due to the shuffle costs. While partitionBy runs, there is a lot of shuffle write and then for the subsequent step there is a lot of shuffle read. Both these steps weren't required in the default version. So, the price paid for better load balancing is the more expensive shuffle costs.