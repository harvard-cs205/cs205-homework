Since points close to one another tend to have similar iteration counts, and thus take similar times to compute, our goal is to choose a partition strategy that does not result in nearby points frequently being in the same partition.

I chose to randomly assign points to one of 100 partitions. Under this strategy, the location of a point has nothing to do with the partition in which it ends up. This method results in a more equitable balancing of work. With default partitioning, the quickest partitions take around 1,000,000 to run, whereas the slowest partitions take around 9,000,000. With random partitioning, the slowest partitions now take around 1,960,000, whereas the fastest take around 2,100,000 (note that the x-axis scaling is different on the two histograms). Thus, the gap between the slowest partition and the fastest is much lower with random partitioning. Furthermore, the distribution of partition times is flatter. With default partitioning, most partitions are fast, but there is a long tail of slower partitions. With random partitioning, the distribution of partition times appears to be roughly normal, with most partitions close to the mean partition time and with few outliers.

The new method does have its tradeoffs. The partitionBy() requires 10-15 seconds, and the partitionBy() and the map() have to write and read about 30 MB to and from memory as a result of shuffling. However, this method does succeed at its goal of more evenly balancing work.