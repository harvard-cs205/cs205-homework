# Your discussion here

Looking at the histograms that plot iteration counts against number of partitions, we see that the distribution for randomized partitioning is much more uniform than that for sequential partitioning. This is because Spark's default partitioning strategy is to break up partitions sequentially and judging from the looks of the Mandelbrot set, there are sequential chunks with low brightness that require less computational intensity/iterations and sequential chunks with high brightness that require much more iterations. Because areas of brightness are pretty concentrated, the sequential strategy is likely to produce many stragglers (the computations on the far right of histogram with most iterations), causing the computation to have poor parallelism.

The custom partitioning strategy by using repartition in 100 tasks gives a much better load balancing by using random assignment rather than sequential partitioning. This is shown by a pretty normal distribution of computation amongst the various partitions, which prevents serious stragglers. However, repartitioning into 100 tasks randomly incurs a shuffle cost which, looking at the SparkUI, occupies more than half of the total runtime. There is a tradeoff here between the shuffling cost which was not present in the sequential partitioning strategy, and we should use random partitioning only if the benefits of balancing load between different worker nodes outweighs the cost of shuffling.