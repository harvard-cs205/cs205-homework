(1) After setting up x-y axis, we use random.shuffle() to randomize them respectively, in the hope of making each partition to do average work.
(2) Seen from Spark UI, among these 100 tasks, the max is 1s, the min is 0.6s, and the median is 0.8s. It balanced the work a lot compared with P2a.py and saved time.
(3) Actually, it's a trade-off between shuffle time and computational time. In this way, although shuffle time increased a little bit, the computational time lowers a lot. 
