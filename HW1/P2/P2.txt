# Your discussion here
P2a_hist.png shows a wide spread of times in the runtime of different partitions.  This is bad becaues it means some partitions will finish fast, while others will still be running.  Therefore you have wasted cycles on the partitions that are done and waiting (at least when you run this on a cluster with multiple cores).  Not only, is it a wide spread of times, most of the partitions run relatively fast compared to the others, and then there is a long tail of just a few partitions that take a long time to finish (these stragglers are computing the areas in the center of the image that are all white pixels).  That means that just a tasks are holding up the whole calculation :/

P2b_hist.png has a much smaller spread of times (1.94-2.12 million as opposed to 0 to 9 million ).  Also, the shape of the graph is much prettier, with a nice bell-curve shape.  Most of the tasks fall in the middle of the range, with just a few falling outside.  I tried partitioning based on columns, rows and a combination of the two, but randomly assigning the tasks appeared to work best at spreading out the pixels that take the longest to compute amongst all of the partitions.

Although the random partitioning strategy is faster overall than the default partitioning, it does add shuffling time to the runtime.  And while in this instance it is still faster to use the random partitioning strategy, there will be instances where the shuffling time counter-balances the gains made by re-partitioning.  It is important to pay attention to this in case the cost of shuffling becomes more than the time it saves.