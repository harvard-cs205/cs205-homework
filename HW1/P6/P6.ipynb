{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from collections import Counter\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Spark1\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'The', u'Project'), Counter({u'Gutenberg': 1})), ((u'in', u'his'), Counter({u'charmed': 1})), ((u'say,', u'that'), Counter({u'never': 1})), ((u'son.', u'So'), Counter({u'say': 1})), ((u'And', u'even'), Counter({u'for': 1}))]\n"
     ]
    }
   ],
   "source": [
    "full_text = sc.textFile(\"./pg100.txt\")\n",
    "#clean data\n",
    "cleaner = lambda word: False if (word.isupper() or word.isdigit() or word == u'' or (word[:-1].isupper() and word[-1] == u'.')) else True\n",
    "full_text = full_text.flatMap(lambda line: line.split(u\" \")).filter(cleaner)\n",
    "\n",
    "#get list of words in order\n",
    "word_list = full_text.collect()\n",
    "#get list of all words that can be first word (of 3)\n",
    "first_words = sc.parallelize(word_list[:-2]) #remove last two words\n",
    "#get list of all words that can be second word (of 3)\n",
    "second_words = sc.parallelize(word_list[1:-1]) #remove first word\n",
    "#get list of all words that can be third word (of 3)\n",
    "third_words = sc.parallelize(word_list[2:]) #remove first two words\n",
    "#third_words = third_words.map(lambda word: [(word,1)]) #add count\n",
    "third_words = third_words.map(lambda word: Counter({word:1})) \n",
    "word_lists = [first_words, second_words, third_words]\n",
    "#zip with index: word => (word, idx), then swap the key and value so we get (idx, word)\n",
    "indexed_word_lists = [lst.zipWithIndex().map(lambda (word, idx): (idx, word)).partitionBy(100) for lst in word_lists]\n",
    "#check copartitioned\n",
    "assert indexed_word_lists[0].partitioner == indexed_word_lists[1].partitioner \n",
    "assert indexed_word_lists[1].partitioner == indexed_word_lists[2].partitioner \n",
    "\n",
    "# get RDD of the form ((first_word, second_word), [(third_word, count_of_third_word)])\n",
    "ugly_format_first_two_words = indexed_word_lists[0].join(indexed_word_lists[1]).join(indexed_word_lists[2])\n",
    "nice_rdd = ugly_format_first_two_words.map(lambda (idx,val): (val[0],val[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'Neighbours', u'and'), [(u'friends,', 1)]),\n",
       " ((u'pence', u'for'), [(u'you;', 1)]),\n",
       " ((u'me.', u'Never'), [(u'did', 1)]),\n",
       " ((u'first,', u'for'), [(u\"mine's\", 1), (u'God', 1)]),\n",
       " ((u'Right.', u'Rom.'), [(u'Why,', 1)]),\n",
       " ((u'can,', u'will'), [(u'put', 1), (u'presume', 1), (u'return.', 1)]),\n",
       " ((u'weapons', u'only'), [(u\"Seem'd\", 1)]),\n",
       " ((u'you', u'anything?'), [(u'No,', 1)]),\n",
       " ((u'loves', u'her;'), [(u'For', 1)]),\n",
       " ((u'good', u'acts'), [(u'whence', 1)]),\n",
       " ((u'beggar.', u'Think'), [(u'not', 1)]),\n",
       " ((u'makes', u'all'), [(u'swift', 1)]),\n",
       " ((u'jaws', u'to'), [(u'open,', 1)]),\n",
       " ((u'To', u'our'),\n",
       "  [(u'gross', 1),\n",
       "   (u'own', 3),\n",
       "   (u'pavilion', 1),\n",
       "   (u'attempts.', 1),\n",
       "   (u'solemnity.', 1),\n",
       "   (u'sport,', 1),\n",
       "   (u'most', 2),\n",
       "   (u'fast-closed', 1),\n",
       "   (u'confusion.', 1)]),\n",
       " ((u'Queen.', u'Nothing'), [(u'at', 1)]),\n",
       " ((u'the', u'jerks'), [(u'of', 1)]),\n",
       " ((u'Clifford;', u'swear'), [(u'as', 1)]),\n",
       " ((u'we', u'do.'), [(u'Certain', 1), (u'What', 1), (u'But', 1), (u'How', 1)]),\n",
       " ((u'gate;', u'And,'), [(u'ere', 1)]),\n",
       " ((u'dearest', u'lord-'), [(u'blest', 1)])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for each (first,second) key get list of (third_word, count) tuples\n",
    "nice_rdd = nice_rdd.reduceByKey(lambda a, b: a + b).mapValues(lambda val: val.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(u'edge', u'invisible,')\n",
      "edge invisible, Cutting a smaller hair than may be marvellously mistook. tell you straight. Exeunt pray you, What is Pyramus? lover,\n",
      "(u'griefs', u'yourself,')\n",
      "griefs yourself, and not love swaggering, by my affection, So far afoot, shall break The cause of distemper? You do the\n",
      "(u'demanded', u'Ere')\n",
      "demanded Ere you go, And be all traitors that do fawn upon, Nay if thou fall'st, Cromwell, Thou fall'st a blessed\n"
     ]
    }
   ],
   "source": [
    "three_starts = nice_rdd.takeSample(True, 3,1)\n",
    "for random_words in three_starts:\n",
    "    first,second = random_words[0]\n",
    "    vals = random_words[1]\n",
    "    phrase = [first,second]\n",
    "    key = (phrase[-2],phrase[-1])\n",
    "    num_words = 2\n",
    "    while num_words < 21:\n",
    "        thirds = [] # possible third words\n",
    "        probs = [] # probabilities of third words\n",
    "        num_thirds = float(len(vals))\n",
    "        total = 0\n",
    "        for word, count in vals:\n",
    "            thirds.append(word)\n",
    "            total += count\n",
    "            probs.append(count/total)\n",
    "        random_third = np.random.choice(thirds, 1,p=probs)\n",
    "        phrase.append(random_third[0])\n",
    "        key = (phrase[-2],phrase[-1])\n",
    "        vals = nice_rdd.map(lambda x: x).lookup(key)[0]\n",
    "        num_words += 1\n",
    "    print ''.join([word + ' ' for word in phrase[:-1]]+[phrase[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
