import pyspark
import re
import random

def word_match(line):
    """Matches all words in a line.
    To be used with flatMap, is it returns a tuple of all the words in a given line.
    """
    # Just one or more letters
    a_word = re.compile(r"\w+")
    # Match all the words in a given line
    matches = a_word.findall(line)
    
    # findall returns a list, so...
    return matches

def filter_words(word):
    """Function that uses regular expressions to filter out words that are made of
    entirely capital letters, or entirely capital letters followed by a period, as per
    the problem statement.
    Note that the way we matched words, \w+, has already filtered out words made of only numbers.
    """
    
    length_of_word = len(word)
    
    # Matches one or more capital letters in a row.
    only_capitals = re.compile(r"[A-Z]{" + str(length_of_word) + "}")
    
    # Matches one or more capital letters in a row followed by a period.
    only_capitals_period = re.compile(r"[A-Z]{" + str(length_of_word-1) + "}\.+")
    
    # Only numbers
    only_numbers = re.compile(r"\d{" + str(length_of_word) + "}")
    
    # if it's a capital...
    caps = only_capitals.match(word)
    
    # if it's a capital followed by a period...
    cap_per = only_capitals_period.match(word)
    
    # if it's a number...
    numbs = only_numbers.match(word)
        
    result = True
    if (caps) or (cap_per) or (numbs):
        result = False
        
    return result

def count_occurrences(words):
    """ Takes an iterable, words, and returns a list
    of tuples of each word that appears in it and the number of times it appears.
    To be used with map after groupByKey.
    """
    
    results = []
    dist_words = set(words)
    orig_words = list(words)
    
    for word in dist_words:
        c = orig_words.count(word)
        results.append((word, c))
        
    return results

def generate_phrase(markov, num_words):
    """Takes a markov chain, assumed to be an RDD of the form described in the problem
    and returns a randomly generated phrase with num_words words. The phrase is generated by
    choosing a pair of words randomly from the RDD, weighting the next word by the relative
    frequencies of occurrences of other words after that pair in Shakespeare, and then
    continues to do so until we have num_words.
    """
    # Our counter
    curr_num_words = 0
    
    # Start us off with a blank phrase
    phrase = []

    # First flag is for with replacement... doesn't matter because we just do one at a time
    # Note that takeSample returns a list, so we just really want one element from it
    next_three = markov.takeSample(True, 1)[0]
    
    # Get the first two words
    word_1 = next_three[0][0]
    word_2 = next_three[0][1]
    
    # Now get the list of possible words
    poss_words = next_three[1]
    
    # And choose accordingly...
    word_3 = weighted_choice(poss_words)
    
    # Add our words to the phrase...
    phrase.append(word_1)
    phrase.append(word_2)
    phrase.append(word_3)
    
    # Make sure we stop at some point
    curr_num_words += 3

    # Now our next pair is the previous two words
    next_pair = (word_2, word_3)

    # And just keep iterating until we are done 
    while (curr_num_words < num_words):
        # Get the list of possible words
        # Do this weird thing because of the Spark error
        next_poss_words = markov.map(lambda x: x).lookup(next_pair)[0]
        
        # Get the actual next word
        next_word = weighted_choice(next_poss_words)

        # Add it to the phrase
        phrase.append(next_word)

        # And shift the pair
        next_pair = (next_pair[1], next_word)

        curr_num_words += 1

                
    # Now we get it to be a string
    phrase = ' '.join(phrase)
    
    # And we are done!
    return phrase

def weighted_choice(choices):
    """ Inspired by http://stackoverflow.com/questions/3679694/a-weighted-version-of-random-choice
    Given a list of tuples, [(x1, c1), (x2, c2), ...]
    Returns some xn from the list chosen randomly but weighted by the counts cn.
    """
    
    # First get the total count
    total = sum(w for c, w in choices)
    
    # Generate some random number between 0 and the total, uniformly
    r = random.uniform(0, total)
    
    # Variable that stores what "range" we are currently in
    upto = 0
    
    # Variable that we will ultimately return
    result = choices[-1][0]
    
    # Loop over all possible choices and weights
    for c, w in choices:
        
        # If our randomly generated number is in the correct range, take it
        if upto + w > r:
            result = c
            
        # Otherwise check the next range
        upto += w
        
    return result

if __name__ == "__main__":

    sc = pyspark.SparkContext(appName='Shakespeare')
    sc.setLogLevel('WARN')
    shakes = sc.textFile('shakespeare.txt', 16)

    # Get all the words that appear in the original text file
    words = shakes.flatMap(word_match)

    # Get rid of non-words according to the problem statement
    words = words.filter(filter_words)

    # Give each word an index and partition the overall RDD.
    # We will preserve this partitioning scheme for all the upcoming manipulations
    numbered_words = words.zipWithIndex().partitionBy(16)

    # Sort by key and repartition into 16 partitions
    # We repartition to improve speed of the join, as we alter the keys
    # so we cannot guarantee the partition scheme will remain stable
    shifted_words = numbered_words.map(lambda (x, y): (y-1, x)).sortByKey().partitionBy(16)
    index_words = numbered_words.map(lambda (x, y): (y, x)).sortByKey().partitionBy(16)

    # Now we join up pairs of words
    # Cache the result so we don't have to repeat it for the join with third words
    indices_and_pairs = index_words.join(shifted_words)
    indices_and_pairs = indices_and_pairs.cache()

    # Now we shift all the words again to find out all possible third words
    # We also partitionBy(16) because we have another join coming up, and we want to
    # minimize shuffling.
    double_shifted_words = numbered_words.map(lambda (x, y): (y-2, x)).sortByKey().partitionBy(16)

    # Join these results to have an RDD that goes as (index, (pair, following_word))
    indexed_pairs_and_followed = indices_and_pairs.join(double_shifted_words)

    # First simply remove the index 
    all_pairs_and_followed = indexed_pairs_and_followed.map(lambda (x, y): (y[0], y[1]))

    # Cache it to improve the speed of the groupByKey
    all_pairs_and_followed = all_pairs_and_followed.cache()

    # Group by key to get a list of following words, with duplicates,
    # for each pair of words
    shakes_rdd_with_dupes = all_pairs_and_followed.groupByKey()

    # Cache our result to speed up the following map
    shakes_rdd_with_dupes = shakes_rdd_with_dupes.cache()

    # Now get rid of the duplicates, but count the occurrences
    # to end up with the markov chain model
    shakes_rdd = shakes_rdd_with_dupes.map(lambda (pair, words): (pair, count_occurrences(words)), preservesPartitioning=True)

    # And now cache our result so we don't have to go back through this computation
    # every time we go to generate a random phrase from our model
    shakes_rdd = shakes_rdd.cache()

    # Now generate our ten phrases using the Markov Chain
    
    with open('P6.txt', 'w') as out_file:
        for ii in range(10):
            phrase = generate_phrase(shakes_rdd, 20)
            print >> out_file, phrase 
