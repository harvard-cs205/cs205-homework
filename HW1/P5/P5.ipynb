{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Version without assuming graph diameter, using accumulator\n",
    "def do_bfs_aws(sc, source_node, hero_graph, n_parts, stop_node):\n",
    "    # Make sure pre-join RDDs are copartitioned\n",
    "    node_hist = (sc.parallelize([(source_node, 0)]).partitionBy(n_parts, hash))\n",
    "    new_nodes = node_hist\n",
    "    hero_graph = hero_graph.partitionBy(n_parts, hash).cache()\n",
    "    hero_filt = hero_graph\n",
    "    assert new_nodes.partitioner == hero_graph.partitioner\n",
    "\n",
    "    # Keep track of whether there are no new nodes touched\n",
    "    accum = sc.accumulator(1)\n",
    "\n",
    "    # Distance corresponding to current iteration\n",
    "    iter_i = 0 \n",
    "    \n",
    "        \n",
    "    while (accum.value > 0 \n",
    "           and new_nodes.filter(lambda x: x[0] == stop_node).count() == 0):\n",
    "        print('Starting iteration ' + str(iter_i))\n",
    "        print(str(new_nodes.count()) + 'new nodes touched')\n",
    "        assert new_nodes.partitioner == hero_filt.partitioner\n",
    "        #         def count_map(K, accum):\n",
    "        #             accum.add(1)\n",
    "        #             return (K, iter_i + 1)\n",
    "        neighbors = (new_nodes.join(hero_filt)\n",
    "                     .flatMap(lambda x: x[1][1])\n",
    "                     .distinct()\n",
    "                     .map(lambda x: (x, iter_i + 1)))\n",
    "\n",
    "        hero_filt = hero_filt.subtractByKey(new_nodes)\n",
    "        hero_filt = hero_filt.partitionBy(n_parts, hash).cache()\n",
    "\n",
    "        # Take away the nodes that were already explored; these are not new\n",
    "        new_nodes = neighbors.subtractByKey(node_hist)\n",
    "        new_nodes = new_nodes.partitionBy(n_parts, hash).cache()\n",
    "\n",
    "        # Use an accumulator for no reason; \n",
    "        # equivalent to performing a count on new_nodes\n",
    "        accum = sc.accumulator(0)\n",
    "        new_nodes.foreach(lambda _: accum.add(1))\n",
    "        node_hist = (node_hist + new_nodes).cache()\n",
    "\n",
    "\n",
    "\n",
    "        iter_i = iter_i + 1\n",
    "    return node_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_short_path(source_node, dest_node, hero_graph, n_parts, sc):\n",
    "    node_hist = do_bfs_aws(sc, source_node, hero_graph, n_parts, dest_node)\n",
    "    # Find what the shortest distance from source to destination is\n",
    "    dest_dist = node_hist.lookup(dest_node)[0]\n",
    "    dest_set = set([dest_node])\n",
    "\n",
    "    shortest_path_options = [[dest_node]]\n",
    "    for dist_i in range(dest_dist - 1, 0, -1):\n",
    "        # Working backward from the destination, \n",
    "        # find all the nodes that are pointing to it\n",
    "        connected_nodes = (hero_graph.filter(\n",
    "            lambda x: len(dest_set.intersection(x[1])) > 0))\n",
    "\n",
    "        # Find all the nodes along the shortest paths \n",
    "        # according to our BFS search \n",
    "        shortest_path_nodes = node_hist.filter(lambda x: x[1] == dist_i)\n",
    "\n",
    "        # The intersection of these are the nodes that are potentially\n",
    "        # along the shortest path between the source and destination nodes.\n",
    "        possible_path_node = connected_nodes.join(shortest_path_nodes)\n",
    "        dest_set = set(possible_path_node.keys().collect())\n",
    "\n",
    "        shortest_path_options.append(list(dest_set))\n",
    "    shortest_path_options.append([source_node])\n",
    "    # Return one possible shortest path\n",
    "    return [options[0] for options in reversed(shortest_path_options)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that finds the number of connected components in a graph\n",
    "def find_connect_comp(sc, hero_graph, n_parts):\n",
    "    import time\n",
    "    # Get list of all heroes\n",
    "    hero_list = hero_graph.keys().cache()\n",
    "    curr_hero_graph = hero_graph\n",
    "    connected_count = 0 # Count the number of connected components\n",
    "    connect_hist = [] # Record information about connected components\n",
    "\n",
    "    while hero_list.count() > 0:\n",
    "        t1 = time.time()\n",
    "        print(connected_count)\n",
    "\n",
    "        # Take a node that hasn't been explored yet\n",
    "        if connected_count == 0:\n",
    "            source_node = hero_list.takeSample(False, 1)[0]\n",
    "        else:\n",
    "            source_node = hero_list.first()\n",
    "\n",
    "        # Do BFS and recover all touched nodes\n",
    "        print('Running BFS...')\n",
    "        search_history = do_bfs_aws(sc, source_node, curr_hero_graph, n_parts, None)\n",
    "        search_history = search_history.partitionBy(n_parts, hash).cache()\n",
    "\n",
    "        # Determine remaining untouched nodes and repeat\n",
    "        print('Pruning nodes...')\n",
    "        hero_list = hero_list.subtract(search_history.keys(), n_parts).cache()\n",
    "\n",
    "        # hero_list = hero_list.partitionBy(n_parts, hash).cache()\n",
    "\n",
    "        # Remove nodes that have been explored \n",
    "        curr_hero_graph = curr_hero_graph.subtractByKey(search_history)\n",
    "        curr_hero_graph = curr_hero_graph.partitionBy(n_parts, hash).cache()\n",
    "\n",
    "        t2 = time.time()\n",
    "        print((search_history.count(), curr_hero_graph.count(), t2 - t1))\n",
    "        connect_hist.append((connected_count, search_history.count(), t2 - t1))\n",
    "        connected_count = connected_count + 1\n",
    "    return connect_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of strings to file\n",
    "def save_str_list(test_strs, save_file):\n",
    "    with open(save_file, 'a') as f:\n",
    "        for word_i, word in enumerate(test_strs):\n",
    "            f.write(word)\n",
    "            if word_i < len(test_strs) - 1:\n",
    "                f.write(' ')\n",
    "            else:\n",
    "                f.write('.\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
