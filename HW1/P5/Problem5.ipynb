{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.ticker as ticker   \n",
    "\n",
    "\n",
    "# setup spark\n",
    "conf = SparkConf().setAppName('BigGraph')\n",
    "sc = SparkContext(conf=conf)\n",
    "#from P5_bfs import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import functools\n",
    "\n",
    "# put the whole process into a function, given an rdd which holds as elements\n",
    "# (v, [v_1, ..., v_d]),  a start node v0 and the spark context corresponding to the rdd\n",
    "def sparkSSSP(context, rdd, v0, vT, max_iter):\n",
    "    \n",
    "    # prepare data structure for single source shortest paths\n",
    "    imaxvalue = sys.maxint\n",
    "\n",
    "    # currently our rdd looks like (v, [v1, v2, ...]). now we map it to a tuple with\n",
    "    # (v, <adj. list.>, disttov0, color, <pred. list>) where disttov0 is 0 for v = v0 and imaxvalue else,\n",
    "    # color = GRAY for v0 and BLACK else\n",
    "    # WHITE means vertex not visited yet\n",
    "    # GRAY means vertex is visited in the next hop\n",
    "    # BLACK mean vertex already visited\n",
    "\n",
    "    # to speedup we use\n",
    "    # WHITE = 2\n",
    "    # GRAY = 1\n",
    "    # BLACK = 0\n",
    "    rdd = rdd.map(lambda x: (x[0], x[1], 0 if x[0] == v0 else imaxvalue, 1 if x[0] == v0 else 2))\n",
    "    \n",
    "    # map to (K, V) form\n",
    "    rdd = rdd.map(lambda x: (x[0], (x[1], x[2], x[3], [])))\n",
    "    \n",
    "    # helper functions for one hop START\n",
    "    \n",
    "    # say we have given (v, [v_1, ..., v_d], d, 'GRAY',  [...])\n",
    "    # this will be expanded to \n",
    "    # (v_1, NULL, d+1, 'GRAY', [..., v])\n",
    "    # ...\n",
    "    # (v_d, NULL, d+1, 'GRAY', [..., v])\n",
    "    # (v, [v_1, ..., v_d], d, 'BLACK', [..., v])\n",
    "    # in the next step we can then call a reducebykey to update distances/adjacency lists\n",
    "    def expandNode(x):\n",
    "        if x[1][2] == 1: # 'GRAY'\n",
    "        # set current node to visited\n",
    "            res = []\n",
    "            res.append( (x[0], (x[1][0], x[1][1], 0, [x[1][3]])) ) # 'BLACK'\n",
    "\n",
    "            # spawn new GRAY nodes\n",
    "            for i in range(0, len(x[1][0])):\n",
    "                res.append( (x[1][0][i], ([], x[1][1] + 1, 1, [x[1][3] + [x[0]]])) ) # 'GRAY'\n",
    "\n",
    "            return tuple(res)\n",
    "        else: \n",
    "            return [x]\n",
    "\n",
    "    # in the next step we combine all tuples for the same key returning\n",
    "    # the minimum distance, longest adjacency list and darkest color\n",
    "    # and minimum path\n",
    "    # the algorithm will determine if there is no gray node left\n",
    "    def reduceNodes(a, b):\n",
    "\n",
    "        # simple solution for only some shortest path\n",
    "        #res = (a[0] if len(a[0]) > len(b[0]) else b[0], min(a[1], b[1]), min(a[2], b[2]), a[3] if a[1] < b[1] else b[3])\n",
    "\n",
    "        #solution for multiple shortest paths is more complicated ==> we store them in a list!\n",
    "        spath_list = [];\n",
    "\n",
    "        spath_lista = a[3]\n",
    "        spath_listb = b[3]\n",
    "\n",
    "        min_dist = 0\n",
    "\n",
    "        if a[1] < b[1]:\n",
    "            spath_list = spath_lista\n",
    "            min_dist = a[1]\n",
    "        elif a[1] > b[1]:\n",
    "            spath_list = spath_listb\n",
    "            min_dist = b[1]\n",
    "        else:\n",
    "            # both are at the same distance --> merge shortest path lists!\n",
    "            spath_list = spath_lista + spath_listb\n",
    "            min_dist = a[1]\n",
    "\n",
    "        # construct tuple\n",
    "        res = (a[0] if len(a[0]) > len(b[0]) else b[0], min_dist, min(a[2], b[2]), spath_list)\n",
    "\n",
    "        # return a tuple of 3 entries\n",
    "        return res\n",
    "    \n",
    "    def countGrayNodes(x, gray_accum):\n",
    "        # inc count of remaining gray nodes by 1!\n",
    "        if x[1][2] == 1:\n",
    "            gray_accum.add(1) \n",
    "\n",
    "        return x\n",
    "\n",
    "    # helper functions END\n",
    "    \n",
    "    # set num_gray_nodes to 1 to start loop (finished when all nodes are visited)\n",
    "    num_remaining_gray_nodes = 1;\n",
    "    num_visited_nodes = 0;\n",
    "    \n",
    "    counter = 0\n",
    "    gray_accum = 0\n",
    "    while num_remaining_gray_nodes > 0 and counter < max_iter:\n",
    "    \n",
    "        # (1) set accumulator for gray nodes to zero\n",
    "        gray_accum = context.accumulator(0)\n",
    "\n",
    "        # filter for gray nodes to make it faster\n",
    "        # ==> look for http://datascience.stackexchange.com/questions/5667/spark-optimally-splitting-a-single-rdd-into-two\n",
    "\n",
    "        # split dataset into one of all the gray nodes (the ones to visit next) and the remaining ones\n",
    "        rddGray = rdd.filter(lambda x: x[1][2] == 1)\n",
    "        rddRest = rdd.filter(lambda x: x[1][2] != 1)\n",
    "\n",
    "        # (2) start map process\n",
    "        rddGray = rddGray.flatMap(expandNode)\n",
    "\n",
    "        rdd = rddRest.union(rddGray)\n",
    "\n",
    "        # (3) then reduce by key\n",
    "        rdd = rdd.reduceByKey(reduceNodes)\n",
    "\n",
    "        # (4) map to count gray nodes\n",
    "        rdd = rdd.map(functools.partial(countGrayNodes, gray_accum=gray_accum))\n",
    "        \n",
    "        # (5) use here a flatmap to solve for all shortest paths (applies only)\n",
    "        rdd = rdd.flatMap(lambda x: [(x[0], (x[1][0], x[1][1], x[1][2], y)) for y in x[1][3]])\n",
    "        \n",
    "        # (6) check if target node vT was reached\n",
    "        rdd_containing_vT = rdd.filter(lambda x: x[0] == vT and x[1][2] == 1)\n",
    "        \n",
    "        # call count to evaluate accumulator correctly\n",
    "        rdd.count()\n",
    "        \n",
    "        # save value of gray node accumulator\n",
    "        num_remaining_gray_nodes = gray_accum.value\n",
    "        num_visited_nodes += num_remaining_gray_nodes \n",
    "        \n",
    "        # leave if target was reached\n",
    "        if rdd_containing_vT.count() > 0:\n",
    "            counter = max_iter\n",
    "            break\n",
    "            \n",
    "        counter += 1\n",
    "    \n",
    "    # map rdd to shortest paths, i.e. return list of paths\n",
    "    #(first is the start, last is the target (can be also left away, as user has this info already))\n",
    "    \n",
    "    rdd = rdd.filter(lambda x: x[0] == vT).map(lambda x: x[1][3] + [x[0]])\n",
    "    \n",
    "    # return number of visited nodes and the rdd\n",
    "    return num_visited_nodes, rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the datasets from P4 as test run...\n",
    "\n",
    "# this function prepares the rdd\n",
    "def prepare_rdd(context, filename):\n",
    "    rdd = context.textFile(filename)\n",
    "\n",
    "    # map string to tuples\n",
    "    rdd = rdd.map(lambda x: x.split(','))\n",
    "    rdd = rdd.map(lambda x: (int(x[0]), int(x[1])))\n",
    "    \n",
    "    # now group s.t. we have for each vertex an adjacency list of nodes\n",
    "    rdd = rdd.groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# load vertex dictionary (is basically the character dictionary inverted)\n",
    "reader = csv.reader(open('characters.csv', 'rb'))\n",
    "vertexDict = dict(reader)\n",
    "\n",
    "keys = vertexDict.values()\n",
    "values = [int(k) for k in vertexDict.keys()]\n",
    "characterDict = dict(zip(keys, values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test code\n",
    "filename = 'edge_list_simple.csv'#'edge_list_simple.csv' # 'edge_list.csv'\n",
    "\n",
    "rdd = prepare_rdd(sc, filename)\n",
    "\n",
    "num_visited_nodes, rdd = sparkSSSP(sc, rdd, 1, 2, 20)\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_visited_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ([2, 5], 0, 0, [])), (5, ([], 1, 1, [1])), (2, ([], 1, 1, [1]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaxvalue = sys.maxint\n",
    "v0 = 1\n",
    "filename = 'edge_list_simple.csv'#'edge_list_simple.csv' # 'edge_list.csv'\n",
    "context = sc\n",
    "\n",
    "rdd = prepare_rdd(sc, filename)\n",
    "\n",
    "rdd = rdd.map(lambda x: (x[0], x[1], 0 if x[0] == v0 else imaxvalue, 1 if x[0] == v0 else 2))\n",
    "    \n",
    "# map to (K, V) form\n",
    "rdd = rdd.map(lambda x: (x[0], (x[1], x[2], x[3], [])))\n",
    "    \n",
    "def expandNode(x):\n",
    "    if x[1][2] == 1: # 'GRAY'\n",
    "    # set current node to visited\n",
    "        res = []\n",
    "        res.append( (x[0], (x[1][0], x[1][1], 0, [x[1][3]])) ) # 'BLACK'\n",
    "\n",
    "        # spawn new GRAY nodes\n",
    "        for i in range(0, len(x[1][0])):\n",
    "            res.append( (x[1][0][i], ([], x[1][1] + 1, 1, [x[1][3] + [x[0]]])) ) # 'GRAY'\n",
    "\n",
    "        return tuple(res)\n",
    "    else: \n",
    "        return [x]\n",
    "\n",
    "# in the next step we combine all tuples for the same key returning\n",
    "# the minimum distance, longest adjacency list and darkest color\n",
    "# and minimum path\n",
    "# the algorithm will determine if there is no gray node left\n",
    "def reduceNodes(a, b):\n",
    "    \n",
    "    # simple solution for only some shortest path\n",
    "    #res = (a[0] if len(a[0]) > len(b[0]) else b[0], min(a[1], b[1]), min(a[2], b[2]), a[3] if a[1] < b[1] else b[3])\n",
    "\n",
    "    #solution for multiple shortest paths is more complicated ==> we store them in a list!\n",
    "    spath_list = [];\n",
    "    \n",
    "    spath_lista = a[3]\n",
    "    spath_listb = b[3]\n",
    "    \n",
    "    min_dist = 0\n",
    "    \n",
    "    if a[1] < b[1]:\n",
    "        spath_list = spath_lista\n",
    "        min_dist = a[1]\n",
    "    elif a[1] > b[1]:\n",
    "        spath_list = spath_listb\n",
    "        min_dist = b[1]\n",
    "    else:\n",
    "        # both are at the same distance --> merge shortest path lists!\n",
    "        spath_list = spath_lista + spath_listb\n",
    "        min_dist = a[1]\n",
    "        \n",
    "    # construct tuple\n",
    "    res = (a[0] if len(a[0]) > len(b[0]) else b[0], min_dist, min(a[2], b[2]), spath_list)\n",
    "\n",
    "    # return a tuple of 3 entries\n",
    "    return res\n",
    "\n",
    "def countGrayNodes(x, gray_accum):\n",
    "    # inc count of remaining gray nodes by 1!\n",
    "    if x[1][2] == 1:\n",
    "        gray_accum.add(1) \n",
    "\n",
    "    return x\n",
    "\n",
    "# helper functions END\n",
    "\n",
    "# set num_gray_nodes to 1 to start loop (finished when all nodes are visited)\n",
    "num_remaining_gray_nodes = 1;\n",
    "num_visited_nodes = 0;\n",
    "\n",
    "counter = 0\n",
    "gray_accum = 0\n",
    "\n",
    "\n",
    "# (1) set accumulator for gray nodes to zero\n",
    "gray_accum = context.accumulator(0)\n",
    "\n",
    "# filter for gray nodes to make it faster\n",
    "# ==> look for http://datascience.stackexchange.com/questions/5667/spark-optimally-splitting-a-single-rdd-into-two\n",
    "\n",
    "# split dataset into one of all the gray nodes (the ones to visit next) and the remaining ones\n",
    "rddGray = rdd.filter(lambda x: x[1][2] == 1).cache()\n",
    "rddRest = rdd.filter(lambda x: x[1][2] != 1)\n",
    "\n",
    "# (2) start map process\n",
    "rddGray = rddGray.flatMap(expandNode)\n",
    "\n",
    "rdd = rddRest.union(rddGray).cache()\n",
    "\n",
    "# (3) then reduce by key\n",
    "rdd = rdd.reduceByKey(reduceNodes).cache()\n",
    "\n",
    "# (4) map to count gray nodes\n",
    "rdd = rdd.map(functools.partial(countGrayNodes, gray_accum=gray_accum))\n",
    "\n",
    "# (5) use here a flatmap to solve for all shortest paths (applies only)\n",
    "rdd = rdd.flatMap(lambda x: [(x[0], (x[1][0], x[1][1], x[1][2], y)) for y in x[1][3]])\n",
    "\n",
    "\n",
    "\n",
    "# call count to evaluate accumulator correctly\n",
    "rdd.count()\n",
    "\n",
    "# save value of gray node accumulator\n",
    "num_remaining_gray_nodes = gray_accum.value\n",
    "num_visited_nodes += num_remaining_gray_nodes \n",
    "        \n",
    "rdd.collect()\n",
    "#test =rdd.take(5)[3]\n",
    "#test\n",
    "\n",
    "#expandNode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_visited_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_visited_nodes, rdd = sparkBFS(sc, rdd, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, ([1, 5, 3, 4], 1, 0, 1)),\n",
       " (3, ([], 2, 0, [1, 5])),\n",
       " (3, ([], 2, 0, [1, 2])),\n",
       " (4, ([], 2, 0, [1, 5])),\n",
       " (4, ([], 2, 0, [1, 2])),\n",
       " (5, ([4, 1, 2, 3], 1, 0, 1))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v0 = [characterDict['CAPTAIN AMERICA'], characterDict['MISS THING/MARY'], characterDict['ORWELL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 1621, 3430]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v0 = [characterDict['CAPTAIN AMERICA'], characterDict['MISS THING/MARY'], characterDict['ORWELL']]\n",
    "filename = 'edge_list.csv'\n",
    "#for i in range(0, len(v0)):\n",
    "i = 0\n",
    "rdd = prepare_rdd(sc, filename)\n",
    "num_visited_nodes, rdd = sparkBFS(sc, rdd, v0[i])\n",
    "\n",
    "print('%s : %d nodes visited' % (vertexDict[str(v0[i])], num_visited_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
