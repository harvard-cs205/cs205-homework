{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.ticker as ticker   \n",
    "\n",
    "\n",
    "# setup spark\n",
    "conf = SparkConf().setAppName('WikiGraph')\n",
    "sc = SparkContext(conf=conf)\n",
    "#from P5_bfs import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to the big data files\n",
    "datapath = '../../../../../courses/CS205_Computing_Foundations/data/'\n",
    "titlespath = datapath + 'titles-sorted.txt'\n",
    "linkspath = datapath + 'links-simple-sorted.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rddTitles = sc.textFile(titlespath)\n",
    "rddGraph = sc.textFile(linkspath)\n",
    "\n",
    "# prepare Graph into adjacency list structure\n",
    "rddGraph = rddGraph.map(lambda x: x.replace(':', '').split(' ')).map(lambda x: (int(x[0]), [int(y) for y in x[1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'!', u'!!', u'!!!', u'!!!!', u'!!!Fuck_You!!!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddTitles.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rddTitles = rddTitles.zipWithIndex().map(lambda x: (x[0], x[1] + 1)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'!', 1), (u'!!', 2), (u'!!!', 3), (u'!!!!', 4), (u'!!!Fuck_You!!!', 5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddTitles.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lookup a title (its index)\n",
    "keyString = 'Kevin_Bacon'\n",
    "v0 = rddTitles.filter(lambda x: x[0] == keyString).collect()[0][1]\n",
    "\n",
    "keyString = 'Harvard_University'\n",
    "vT = rddTitles.filter(lambda x: x[0] == keyString).collect()[0][1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2729536"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2152782"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute shortest path!\n",
    "from P5_sssp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_visited_nodes, rddPaths = sparkSSSP(sc, rddGraph, v0, vT, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultPaths = rddPaths.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2729536, 3229511, 2152782],\n",
       " [2729536, 4625677, 2152782],\n",
       " [2729536, 2578703, 2152782],\n",
       " [2729536, 5114592, 2152782],\n",
       " [2729536, 1124925, 2152782]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddLookup = sc.parallelize(resultPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2729536, 3229511, 2152782],\n",
       " [2729536, 4625677, 2152782],\n",
       " [2729536, 2578703, 2152782],\n",
       " [2729536, 5114592, 2152782],\n",
       " [2729536, 1124925, 2152782]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddLookup.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Kevin_Bacon', u'Harvard_University', u'Marisa_Silver'],\n",
       " [u'Kevin_Bacon', u'Six_degrees_of_separation', u'Harvard_University'],\n",
       " [u'Kevin_Bacon', u'Harvard_University', u'John_Lithgow'],\n",
       " [u'Time_(magazine)', u'Kevin_Bacon', u'Harvard_University'],\n",
       " [u'Kevin_Bacon', u'College_Bowl', u'Harvard_University']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rddLookup.zipWithIndex().flatMap(lambda x: [(y, x[1]+1) for y in x[0]]).join(rddTitles.map(lambda x: (x[1], x[0]))).map(lambda x: (x[1][0], x[1][1])).groupByKey().map(lambda x: list(x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2729536, (1, 1)),\n",
       " (3229511, (1, 2)),\n",
       " (2152782, (1, 3)),\n",
       " (2729536, (2, 1)),\n",
       " (4625677, (2, 2)),\n",
       " (2152782, (2, 3)),\n",
       " (2729536, (3, 1)),\n",
       " (2578703, (3, 2)),\n",
       " (2152782, (3, 3)),\n",
       " (2729536, (4, 1)),\n",
       " (5114592, (4, 2)),\n",
       " (2152782, (4, 3)),\n",
       " (2729536, (5, 1)),\n",
       " (1124925, (5, 2)),\n",
       " (2152782, (5, 3))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddLookup.zipWithIndex().flatMap(lambda x: [(y, (x[1]+1, i)) for i, y in enumerate(x[0], 1)])rddLookup.zipWithIndex().flatMap(lambda x: [(y, (x[1]+1, i)) for i, y in enumerate(x[0], 1)]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for better performance on join, \n",
    "rddLookup.cache()\n",
    "nodesOfInterest = rddLookup.flatMap(lambda x: x).distinct().collect()\n",
    "selectedTitles = rddTitles.filter(lambda x: x[1] in nodesOfInterest).map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Kevin_Bacon', u'Marisa_Silver', u'Harvard_University'),\n",
       " (u'Kevin_Bacon', u'Six_degrees_of_separation', u'Harvard_University'),\n",
       " (u'Kevin_Bacon', u'John_Lithgow', u'Harvard_University'),\n",
       " (u'Kevin_Bacon', u'Time_(magazine)', u'Harvard_University'),\n",
       " (u'Kevin_Bacon', u'College_Bowl', u'Harvard_University')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddLookup.zipWithIndex() \\\n",
    "         .flatMap(lambda x: [(y, (x[1]+1, i)) for i, y in enumerate(x[0], 1)]) \\\n",
    "         .join(selectedTitles) \\\n",
    "         .map(lambda x: (x[1][0][0], (x[1][0][1], x[1][1]))) \\\n",
    "         .groupByKey().map(lambda x: zip(*sorted(list(x[1])))[1]).collect()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = [(u'Kevin_Bacon', u'Marisa_Silver', u'Harvard_University'),\n",
    " (u'Kevin_Bacon', u'Six_degrees_of_separation', u'Harvard_University'),\n",
    " (u'Kevin_Bacon', u'John_Lithgow', u'Harvard_University'),\n",
    " (u'Kevin_Bacon', u'Time_(magazine)', u'Harvard_University'),\n",
    " (u'Kevin_Bacon', u'College_Bowl', u'Harvard_University')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Kevin_Bacon', u'Marisa_Silver', u'Harvard_University'),\n",
       " (u'Kevin_Bacon', u'Six_degrees_of_separation', u'Harvard_University'),\n",
       " (u'Kevin_Bacon', u'John_Lithgow', u'Harvard_University'),\n",
       " (u'Kevin_Bacon', u'Time_(magazine)', u'Harvard_University'),\n",
       " (u'Kevin_Bacon', u'College_Bowl', u'Harvard_University')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startTitle  = 'Kevin_Bacon'\n",
    "targetTitle = 'Harvard_University'\n",
    "\n",
    "with open('output.txt', 'wb') as f:\n",
    "\tf.write('Shortest paths from %s to %s:\\n\\n' % (startTitle, targetTitle))\n",
    "\tfor path in test:\n",
    "            path = list(path) \n",
    "            f.write(path[0])\n",
    "            for node in path[1:]:\n",
    "                f.write(' -> ' + node)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Connected Components for Wikigraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform PEGASUS algorithm after ...\n",
    "def connectedComponents(rddIn):\n",
    "    \n",
    "    # transform to structure (init)\n",
    "    rdd = rddIn.map(lambda x: (x[0], (x[1], x[0])))\n",
    "    maxNodeID = rdd.map(lambda x: x[0]).max()\n",
    "    minNodeID = rdd.map(lambda x: x[0]).min()\n",
    "    \n",
    "    # helper function to check whether the partitioning in connected components changed\n",
    "    def checkForPartitionChange(newSizes, oldSizes):\n",
    "        partitionChanged = False\n",
    "\n",
    "        # quick check if length of the dict is different to previous step\n",
    "        # the partitions changed!\n",
    "        if len(oldSizes.items()) != len(newSizes.items()):\n",
    "            partitionChanged = True\n",
    "        else:\n",
    "            # the size did not change, but what about some internal shifting?\n",
    "            # ==> check for each connected component if size changed!\n",
    "            for ID in range(minNodeID, maxNodeID+1):\n",
    "                # check if for old / new the current ID exists and is equal\n",
    "                # ==> if not, a change happened!\n",
    "                try:\n",
    "                    sizeOld = oldSizes[ID]\n",
    "                    sizeNew = newSizes[ID]\n",
    "\n",
    "                    if sizeOld != sizeNew:\n",
    "                        partitionChanged = True\n",
    "                        break\n",
    "                except KeyError:\n",
    "                    partitionChanged = True\n",
    "                    break\n",
    "\n",
    "        return partitionChanged\n",
    "    \n",
    "    \n",
    "    # this is the algorithm (Pegasus after ...)\n",
    "    done = False\n",
    "    \n",
    "    counter = 0;\n",
    "    while not done:\n",
    "        # compute sizes of connected components\n",
    "        rddComponents = rdd.map(lambda x: x[1][1])\n",
    "        oldSizes = rddComponents.countByValue()\n",
    "\n",
    "\n",
    "        # mapper / expander\n",
    "        rdd = rdd.flatMap(lambda x: [x] + [(y, ([], x[1][1])) for y in x[1][0]])\n",
    "\n",
    "        # reducer (as there is only !one! element with the whole adj. list, we can speed it up here!)\n",
    "        rdd = rdd.reduceByKey(lambda a,b: (a[0] + b[0], min(a[1], b[1])))\n",
    "\n",
    "        # determine if number of partitions changed! (0 is a dummy value to construct an pair rdd)\n",
    "        rddComponents = rdd.map(lambda x: x[1][1])\n",
    "        newSizes = rddComponents.countByValue()\n",
    "\n",
    "        # if partition changed, one more round!\n",
    "        done = not checkForPartitionChange(newSizes, oldSizes)\n",
    "\n",
    "        oldSizes = newSizes\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "    # reconstruct connected components\n",
    "    componentList = rdd.map(lambda x: (x[1][1], x[0])).sortByKey().groupByKey().map(lambda x: sorted(list(x[1]))).collect()\n",
    "\n",
    "    return counter, componentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all in one function\n",
    "def directedGraph2symmetric(rdd):\n",
    "    \n",
    "    return rdd.flatMap(lambda x: [x] + [(y, [x[0]]) for y in x[1]]) \\\n",
    "              .sortByKey() \\\n",
    "              .reduceByKey(lambda a,b: a+b) \\\n",
    "              .map(lambda x: (x[0], list(set(x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all in one function\\\n",
    "def directedGraph2linked(rdd, includeSingleComponents=True):\n",
    "    rddSingleNodes = None\n",
    "    if includeSingleComponents:\n",
    "        rddSingleNodes = rdd.map(lambda x: (x[0], []))\n",
    "        \n",
    "    rdd = rdd.flatMap(lambda x: [(x[0], y) for y in x[1]])\n",
    "    rdd = rdd.map(lambda x: ((min(x[0], x[1]), max(x[0], x[1])), 1))\n",
    "    rdd = rdd.reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] > 1).map(lambda x: x[0])\n",
    "    rdd = rdd.flatMap(lambda x: [(x[0], [x[1]]), (x[1], [x[0]])])\n",
    "    \n",
    "    if includeSingleComponents:\n",
    "        # if desired, join with single nodes\n",
    "        rdd = rdd.union(rddSingleNodes)\n",
    "\n",
    "    rdd = rdd.reduceByKey(lambda a, b: a+b)\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment 1 - symmetric graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to the big data files\n",
    "datapath = '../../../../../courses/CS205_Computing_Foundations/data/'\n",
    "titlespath = datapath + 'titles-sorted.txt'\n",
    "linkspath = datapath + 'links-simple-sorted.txt'\n",
    "\n",
    "rddTitles = sc.textFile(titlespath)\n",
    "rddGraph = sc.textFile(linkspath)\n",
    "\n",
    "# prepare Graph into adjacency list structure\n",
    "rddGraph = rddGraph.map(lambda x: x.replace(':', '').split(' ')).map(lambda x: (int(x[0]), [int(y) for y in x[1:]])).cache()\n",
    "# prepare Title dictionary\n",
    "rddTitles = rddTitles.zipWithIndex().map(lambda x: (x[0], x[1] + 1)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 6.0 failed 1 times, most recent failure: Lost task 2.0 in stage 6.0 (TID 82, localhost): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:113)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:101)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:97)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:118)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:373)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:113)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:101)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:97)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:118)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fc73c3bd2b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert to symmetric Graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrddUD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectedGraph2symmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrddGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmappedGraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrddUD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 6.0 failed 1 times, most recent failure: Lost task 2.0 in stage 6.0 (TID 82, localhost): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:113)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:101)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:97)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:118)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:373)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:113)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:101)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:97)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:118)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# convert to symmetric Graph\n",
    "rddUD = directedGraph2symmetric(rddGraph)\n",
    "mappedGraph = rddUD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter, cList = connectedComponents(rddUD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# experiment - links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to the big data files\n",
    "datapath = '../../../../../courses/CS205_Computing_Foundations/data/'\n",
    "titlespath = datapath + 'titles-sorted.txt'\n",
    "linkspath = datapath + 'links-simple-sorted.txt'\n",
    "\n",
    "rddTitles = sc.textFile(titlespath)\n",
    "rddGraph = sc.textFile(linkspath)\n",
    "\n",
    "# prepare Graph into adjacency list structure\n",
    "rddGraph = rddGraph.map(lambda x: x.replace(':', '').split(' ')).map(lambda x: (int(x[0]), [int(y) for y in x[1:]]))\n",
    "# prepare Title dictionary\n",
    "rddTitles = rddTitles.zipWithIndex().map(lambda x: (x[0], x[1] + 1)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to symmetric Graph\n",
    "rddUD2 = directedGraph2linked(rddGraph, True)\n",
    "\n",
    "counter2, cList2 = connectedComponents(rddUD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
