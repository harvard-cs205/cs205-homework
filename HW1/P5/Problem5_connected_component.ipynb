{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# shortest path code is in this module\n",
    "from P5_sssp import *\n",
    "\n",
    "# setup spark\n",
    "conf = SparkConf().setAppName('WikiGraph')\n",
    "sc = SparkContext(conf=conf, pyFiles=['P5_sssp.py'])\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare test graph\n",
    "# graph should have no jumps in node IDs! (maybe remap before!)\n",
    "testgraph = [(1, [2, 3]), (2, [1, 3]), (3, [1,2,4]), (4, [3]), (5, [6, 7]), (6, [5]), (7, [5])]\n",
    "rdd = sc.parallelize(testgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform to structure (init)\n",
    "rddA = rdd.map(lambda x: (x[0], (x[1], x[0])))\n",
    "maxNodeID = rddA.map(lambda x: x[0]).max()\n",
    "minNodeID = rddA.map(lambda x: x[0]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.map(lambda x: x[0]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ([2, 3], 1)),\n",
       " (2, ([1, 3], 2)),\n",
       " (3, ([1, 2, 4], 3)),\n",
       " (4, ([3], 4)),\n",
       " (5, ([6, 7], 5)),\n",
       " (6, ([5], 6)),\n",
       " (7, ([5], 7))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddA.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to check whether the partitioning in connected components changed\n",
    "def checkForPartitionChange(newSizes, oldSizes):\n",
    "    partitionChanged = False\n",
    "\n",
    "    # quick check if length of the dict is different to previous step\n",
    "    # the partitions changed!\n",
    "    if len(oldSizes.items()) != len(newSizes.items()):\n",
    "        partitionChanged = True\n",
    "    else:\n",
    "        # the size did not change, but what about some internal shifting?\n",
    "        # ==> check for each connected component if size changed!\n",
    "        for ID in range(minNodeID, maxNodeID+1):\n",
    "            # check if for old / new the current ID exists and is equal\n",
    "            # ==> if not, a change happened!\n",
    "            try:\n",
    "                sizeOld = oldSizes[ID]\n",
    "                sizeNew = newSizes[ID]\n",
    "\n",
    "                if sizeOld != sizeNew:\n",
    "                    partitionChanged = True\n",
    "                    break\n",
    "            except KeyError:\n",
    "                partitionChanged = True\n",
    "                break\n",
    "                \n",
    "    return partitionChanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rdd = rddA\n",
    "\n",
    "\n",
    "# this is the algorithm (Pegasus after ...)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # compute sizes of connected components\n",
    "    rddComponents = rdd.map(lambda x: x[1][1])\n",
    "    oldSizes = rddComponents.countByValue()\n",
    "\n",
    "\n",
    "    # mapper / expander\n",
    "    rdd = rdd.flatMap(lambda x: [x] + [(y, ([], x[1][1])) for y in x[1][0]])\n",
    "\n",
    "    # reducer (as there is only !one! element with the whole adj. list, we can speed it up here!)\n",
    "    rdd = rdd.reduceByKey(lambda a,b: (a[0] + b[0], min(a[1], b[1])))\n",
    "\n",
    "    # determine if number of partitions changed! (0 is a dummy value to construct an pair rdd)\n",
    "    rddComponents = rdd.map(lambda x: x[1][1])\n",
    "    newSizes = rddComponents.countByValue()\n",
    "\n",
    "    # if partition changed, one more round!\n",
    "    done = not checkForPartitionChange(newSizes, oldSizes)\n",
    "\n",
    "    oldSizes = newSizes\n",
    "\n",
    "# reconstruct connected components\n",
    "componentList = rdd.map(lambda x: (x[1][1], x[0])).sortByKey().groupByKey().map(lambda x: sorted(list(x[1]))).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform PEGASUS algorithm after ...\n",
    "def connectedComponents(rddIn):\n",
    "    \n",
    "    # transform to structure (init)\n",
    "    rdd = rddIn.map(lambda x: (x[0], (x[1], x[0])))\n",
    "    maxNodeID = rdd.map(lambda x: x[0]).max()\n",
    "    minNodeID = rdd.map(lambda x: x[0]).min()\n",
    "    \n",
    "    # helper function to check whether the partitioning in connected components changed\n",
    "    def checkForPartitionChange(newSizes, oldSizes):\n",
    "        partitionChanged = False\n",
    "\n",
    "        # quick check if length of the dict is different to previous step\n",
    "        # the partitions changed!\n",
    "        if len(oldSizes.items()) != len(newSizes.items()):\n",
    "            partitionChanged = True\n",
    "        else:\n",
    "            # the size did not change, but what about some internal shifting?\n",
    "            # ==> check for each connected component if size changed!\n",
    "            for ID in range(minNodeID, maxNodeID+1):\n",
    "                # check if for old / new the current ID exists and is equal\n",
    "                # ==> if not, a change happened!\n",
    "                try:\n",
    "                    sizeOld = oldSizes[ID]\n",
    "                    sizeNew = newSizes[ID]\n",
    "\n",
    "                    if sizeOld != sizeNew:\n",
    "                        partitionChanged = True\n",
    "                        break\n",
    "                except KeyError:\n",
    "                    partitionChanged = True\n",
    "                    break\n",
    "\n",
    "        return partitionChanged\n",
    "    \n",
    "    \n",
    "    # this is the algorithm (Pegasus after http://arxiv.org/pdf/1203.5387.pdf)\n",
    "    done = False\n",
    "    \n",
    "    counter = 0;\n",
    "    while not done:\n",
    "        # compute sizes of connected components\n",
    "        rddComponents = rdd.map(lambda x: x[1][1])\n",
    "        oldSizes = rddComponents.countByValue()\n",
    "\n",
    "\n",
    "        # mapper / expander\n",
    "        rdd = rdd.flatMap(lambda x: [x] + [(y, ([], x[1][1])) for y in x[1][0]])\n",
    "\n",
    "        # reducer (as there is only !one! element with the whole adj. list, we can speed it up here!)\n",
    "        rdd = rdd.reduceByKey(lambda a,b: (a[0] + b[0], min(a[1], b[1])))\n",
    "        \n",
    "        # determine if number of partitions changed! (0 is a dummy value to construct an pair rdd)\n",
    "        rddComponents = rdd.map(lambda x: x[1][1])\n",
    "        newSizes = rddComponents.countByValue()\n",
    "\n",
    "        # if partition changed, one more round!\n",
    "        done = not checkForPartitionChange(newSizes, oldSizes)\n",
    "\n",
    "        oldSizes = newSizes\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "    # reconstruct connected components\n",
    "    componentList = rdd.map(lambda x: (x[1][1], x[0])).sortByKey().groupByKey().map(lambda x: sorted(list(x[1]))).collect()\n",
    "\n",
    "    return counter, componentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test run on rdd!\n",
    "rddTest = sc.parallelize(testgraph)\n",
    "\n",
    "cList = connectedComponents(rddTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, [[1, 2, 3, 4], [5, 6, 7]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test run for Marvel Graph\n",
    "\n",
    "# this function prepares the rdd\n",
    "def prepare_rdd(filename):\n",
    "    rdd = sc.textFile(filename)\n",
    "\n",
    "    # map string to tuples\n",
    "    rdd = rdd.map(lambda x: x.split(','))\n",
    "    rdd = rdd.map(lambda x: (int(x[0]), int(x[1])))\n",
    "    \n",
    "    # now group s.t. we have for each vertex an adjacency list of nodes\n",
    "    rdd = rdd.groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test code\n",
    "filename = 'edge_list.csv'#'edge_list_simple.csv' # 'edge_list.csv'\n",
    "\n",
    "rdd = prepare_rdd(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now find connected components!\n",
    "num_iterations, cList = connectedComponents(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversion function for directedGraph to undirected one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make each connection symmetric\n",
    "\n",
    "# prepare test graph\n",
    "# graph should have no jumps in node IDs! (maybe remap before!)\n",
    "testgraph = [(1, [2, 3]), (2, [3]), (3, [2]), (4, [3]), (5, []), (6, [5]), (7, [5])]\n",
    "rdd = sc.parallelize(testgraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = rdd.flatMap(lambda x: [x] + [(y, [x[0]]) for y in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = rdd.reduceByKey(lambda a,b: list(set(a) | set(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, [3]),\n",
       " (1, [2, 3]),\n",
       " (5, [6, 7]),\n",
       " (2, [1, 3]),\n",
       " (6, [5]),\n",
       " (3, [1, 2, 4]),\n",
       " (7, [5])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all in one function\n",
    "def directedGraph2symmetric(rdd):\n",
    "    return rdd.flatMap(lambda x: [x] + [(y, [x[0]]) for y in x[1]]).reduceByKey(lambda a,b: a+b).map(lambda x: (x[0], list(set(x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, [3]),\n",
       " (1, [2, 3]),\n",
       " (5, [6, 7]),\n",
       " (2, [1, 3]),\n",
       " (6, [5]),\n",
       " (3, [1, 2, 4]),\n",
       " (7, [5])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test code for the function\n",
    "testgraph = [(1, [2, 3]), (2, [3]), (3, [2]), (4, [3]), (5, []), (6, [5]), (7, [5])]\n",
    "rdd = sc.parallelize(testgraph)\n",
    "directedGraph2symmetric(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [2, 3]), (2, [3]), (3, [2, 4]), (4, [3]), (5, []), (6, [5]), (7, [5])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test code for the function\n",
    "testgraph = [(1, [2, 3]), (2, [3]), (3, [2, 4]), (4, [3]), (5, []), (6, [5]), (7, [5])]\n",
    "rdd = sc.parallelize(testgraph)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rddSingleNodes = rdd.map(lambda x: (x[0], []))\n",
    "rdd = rdd.flatMap(lambda x: [(x[0], y) for y in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda x: ((min(x[0], x[1]), max(x[0], x[1])), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), 1),\n",
       " ((1, 3), 1),\n",
       " ((2, 3), 1),\n",
       " ((2, 3), 1),\n",
       " ((3, 4), 1),\n",
       " ((3, 4), 1),\n",
       " ((5, 6), 1),\n",
       " ((5, 7), 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = rdd.reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] > 1).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.flatMap(lambda x: [(x[0], [x[1]]), (x[1], [x[0]])])\n",
    "# if desired, join with single nodes\n",
    "rdd = rdd.union(rddSingleNodes)\n",
    "\n",
    "rdd = rdd.reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, []), (2, [3]), (3, [2, 4]), (4, [3]), (5, []), (6, []), (7, [])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WICHTIG!!! Bei dem zweiten, unbedingt die Zusammenhangskomponenten mit Groesse 1 zuerst rausfiltern! Damit ist der Algorithmus schneller!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all in one function\\\n",
    "def directedGraph2linked(rdd, includeSingleComponents=True):\n",
    "    rddSingleNodes = None\n",
    "    if includeSingleComponents:\n",
    "        rddSingleNodes = rdd.map(lambda x: (x[0], []))\n",
    "        \n",
    "    rdd = rdd.flatMap(lambda x: [(x[0], y) for y in x[1]])\n",
    "    rdd = rdd.map(lambda x: ((min(x[0], x[1]), max(x[0], x[1])), 1))\n",
    "    rdd = rdd.reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] > 1).map(lambda x: x[0])\n",
    "    rdd = rdd.flatMap(lambda x: [(x[0], [x[1]]), (x[1], [x[0]])])\n",
    "    \n",
    "    if includeSingleComponents:\n",
    "        # if desired, join with single nodes\n",
    "        rdd = rdd.union(rddSingleNodes)\n",
    "\n",
    "    rdd = rdd.reduceByKey(lambda a, b: a+b)\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, [3]), (2, [3]), (3, [2, 4])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test code for the function\n",
    "testgraph = [(1, [2, 3]), (2, [3]), (3, [2, 4]), (4, [3]), (5, []), (6, [5]), (7, [5])]\n",
    "rdd = sc.parallelize(testgraph)\n",
    "rdd.collect()\n",
    "\n",
    "directedGraph2linked(rdd, False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wikipedia Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to the big data files\n",
    "datapath = '../../../../../courses/CS205_Computing_Foundations/data/'\n",
    "titlespath = datapath + 'titles-sorted.txt'\n",
    "linkspath = datapath + 'links-simple-sorted.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to prepare two rdds, one holding the graph, the other for later use as a dictionary\n",
    "def prepareWikiGraph(titlefile, linksfile):\n",
    "\trddTitles = sc.textFile(titlefile)\n",
    "\trddGraph = sc.textFile(linksfile)\n",
    "\n",
    "\t# title file has structure v: v0, ...., v_d\n",
    "\t# simple mapping will give the rdd structure\n",
    "\t# (v, [v_1, ..., v_d]) as needed by the sssp algorithm\n",
    "\trddGraph = rddGraph.map(lambda x: x.replace(':', '').split(' ')) \\\n",
    "\t\t\t\t\t   .map(lambda x: (int(x[0]), [int(y) for y in x[1:]]))\n",
    "\n",
    "\t# note that for the wikigraph everything is 1-indexed\n",
    "\t# dictionary has structure ('a wiki title', 23)\n",
    "\trddTitles = rddTitles.zipWithIndex().map(lambda x: (x[0], x[1] + 1)).cache()\n",
    "\treturn rddGraph, rddTitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddGraph, rddTitles = prepareWikiGraph(titlespath, linkspath) \n",
    "\n",
    "# cache graph for better performance\n",
    "rddGraph = rddGraph.cache()\n",
    "\n",
    "# first compute connected components for the linked graph (it is smaller)\n",
    "rddLinked = directedGraph2linked(rddGraph, False)\n",
    "\n",
    "# now find connected components!\n",
    "num_iterations, cList = connectedComponents(rddGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cList) # add to this number 1 -components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
