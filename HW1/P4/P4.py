import pyspark
from pyspark import SparkContext, SparkConf
sc = SparkContext()



data = sc.textFile('source.csv')


def split_character_issue(s):
	'''split the character issue pair'''
	split_s = s.split('"')
	s_character = split_s[1]
	s_issue = split_s[3]
	return (s_character, s_issue)
split_data = data.map(split_character_issue)


def swap_character_issue(tpl):
	'''swap the order to first issue then character'''
	return (tpl[1], [tpl[0]])   #notice character is made into a list
swap_data = split_data.map(swap_character_issue)


issue_data = swap_data.reduceByKey(lambda a,b: a+b)


def extractEdges(mytuple):
	'''create a one-to-one map (an edge) for each pair of characters 
	   corresponding to an issue'''
	graph_edges = []
	char_list = mytuple[1]
	for i in range(len(char_list)):
		for j in range(len(char_list)):
			graph_edges.append( (char_list[i],char_list[j]) )
	return graph_edges
#Notice we use flatMap instead of map to combine outcomes of different keys            
edges_data = issue_data.flatMap(extractEdges)   


#get rid of the edges with a character connecting to itself
edges_data = edges_data.filter(lambda tpl: tpl[0]!=tpl[1])


#change the second element of the tuple to list
edges_data = edges_data.map(lambda tpl: (tpl[0], [tpl[1]]))


#key is a character and value is a list of characters connecting to it 
#this is also the final graph
graph_data = edges_data.reduceByKey(lambda a,b: a+b)


#a pair of characters may exist simultaneously in the an issue
#so a character may show up more than once in the value list corresponding to a key
#this step is for getting rid of the repetitions generated by "reduceByKey"
final_data = graph_data.map(lambda tpl: (tpl[0],list(set(tpl[1]))) )





from P4_bfs import *
print 'CAPTAIN AMERICA:'
bfs(final_data, 'CAPTAIN AMERICA')
print '\n'
print 'MISS THING/MARY:'
bfs(final_data, 'MISS THING/MARY')
print'\n'
print'ORWELL:'
bfs(final_data, 'ORWELL')
