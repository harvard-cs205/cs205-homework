{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hero_graph(text_path, sc, n_parts):\n",
    "\n",
    "    # Load marvel comic data\n",
    "    dat = sc.textFile(text_path)\n",
    "\n",
    "    # Remove quotations and split issue & hero name\n",
    "    dat_split = dat.map(lambda x: x[1:-1].split('\",\"'))\n",
    "\n",
    "    # Mapping between issue->hero\n",
    "    dat_comic = dat_split.map(lambda x: (x[1], x[0]))\n",
    "    dat_comic = dat_comic.partitionBy(n_parts).cache()\n",
    "\n",
    "    comic_key = dat_comic.combineByKey(lambda x: {x}, \n",
    "                                       lambda a, b: a.union({b}), \n",
    "                                       lambda a, b: a.union(b))\n",
    "    assert dat_comic.partitioner == comic_key.partitioner\n",
    "\n",
    "    comic_hero_join = dat_comic.join(comic_key).map(lambda x: x[1])\n",
    "    comic_hero_join = comic_hero_join.partitionBy(n_parts)\n",
    "\n",
    "    hero_graph = comic_hero_join.combineByKey(lambda x: x, \n",
    "                                     lambda a, b: a.union(b), \n",
    "                                     lambda a, b: a.union(b))\n",
    "    hero_graph = hero_graph.map(lambda KV: (KV[0], KV[1] - {KV[0]}), True)\n",
    "    assert hero_graph.partitioner == comic_key.partitioner\n",
    "    return hero_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Version assuming graph diameter = 10\n",
    "def do_bfs1(sc, source_node, hero_graph, n_parts):\n",
    "    curr_nodes = sc.parallelize([(source_node, 0)], n_parts)\n",
    "\n",
    "    graph_diam = 10\n",
    "    for iter_i in range(graph_diam):\n",
    "        neighbors = (curr_nodes.join(hero_graph)\n",
    "                     .flatMap(lambda x: x[1][1])\n",
    "                     .map(lambda x: (x, iter_i + 1)))\n",
    "        new_nodes = neighbors.subtractByKey(curr_nodes, n_parts).cache()\n",
    "        curr_nodes = ((curr_nodes + new_nodes)\n",
    "                      .repartition(n_parts).cache())\n",
    "    return curr_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Version without assuming graph diameter, using accumulator\n",
    "def do_bfs2(sc, source_node, hero_graph, n_parts, stop_node):\n",
    "    # Make sure pre-join RDDs are copartitioned\n",
    "    node_hist = (sc.parallelize([(source_node, 0)])\n",
    "                 .partitionBy(n_parts, hash))\n",
    "    new_nodes = node_hist\n",
    "    hero_graph = hero_graph.partitionBy(n_parts, hash).cache()\n",
    "    hero_filt = hero_graph\n",
    "    assert new_nodes.partitioner == hero_graph.partitioner\n",
    "\n",
    "    # Keep track of whether there are no new nodes touched\n",
    "    accum = sc.accumulator(1)\n",
    "\n",
    "    # Distance corresponding to current iteration\n",
    "    iter_i = 0 \n",
    "\n",
    "    while (accum.value > 0 \n",
    "           and new_nodes.filter(lambda x: x[0] == stop_node).count() == 0):\n",
    "        print('Starting iteration {}'.format(iter_i))\n",
    "        assert new_nodes.partitioner == hero_filt.partitioner\n",
    "        #         def count_map(K, accum):\n",
    "        #             accum.add(1)\n",
    "        #             return (K, iter_i + 1)\n",
    "        neighbors = (new_nodes.join(hero_filt)\n",
    "                     .flatMap(lambda x: x[1][1])\n",
    "                     .distinct()\n",
    "                     .map(lambda x: (x, iter_i + 1)))\n",
    "\n",
    "        hero_filt = hero_filt.subtractByKey(new_nodes)\n",
    "        hero_filt = hero_filt.partitionBy(n_parts, hash).cache()\n",
    "\n",
    "        # Take away the nodes that were already explored; these are not new\n",
    "        new_nodes = neighbors.subtractByKey(node_hist)\n",
    "        new_nodes = new_nodes.partitionBy(n_parts, hash).cache()\n",
    "\n",
    "        # Use an accumulator for no reason; \n",
    "        # equivalent to performing a count on new_nodes\n",
    "        accum = sc.accumulator(0)\n",
    "        new_nodes.foreach(lambda _: accum.add(1))\n",
    "        node_hist = (node_hist + new_nodes).cache()\n",
    "\n",
    "\n",
    "\n",
    "        iter_i = iter_i + 1\n",
    "    return node_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
