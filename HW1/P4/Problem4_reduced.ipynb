{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.ticker as ticker   \n",
    "import csv\n",
    "\n",
    "\n",
    "# setup spark\n",
    "conf = SparkConf().setAppName('Graph Processing')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "from P4_bfs import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Conversion from MarvelGraph to adj. list representation\n",
    "# comicRDD = sc.textFile('source.csv').map(lambda x: x.split('\",\"')).map(lambda x: (x[0][1:], x[1][:len(x[1])-1]))\n",
    "\n",
    "# # create Dict (last part can be removed as it does only make indices 1-based)\n",
    "# dictRDD = comicRDD.map(lambda x: x[0]).distinct().sortBy(lambda x: x).zipWithIndex().map(lambda x: (x[0], x[1]+1))\n",
    "\n",
    "# # convert Character to integer for later join!\n",
    "# comicRDD = comicRDD.join(dictRDD).map(lambda x: (x[1][0], x[1][1])).cache()\n",
    "\n",
    "# # join on Comic Issue & remove all reflexive edges\n",
    "# graphRDD = comicRDD.join(comicRDD).map(lambda x: (x[1][0], x[1][1])).filter(lambda x: x[0] != x[1])\n",
    "\n",
    "# # now group s.t. we have for each vertex an adjacency list of nodes\n",
    "# graphRDD = graphRDD.groupByKey().map(lambda x: (x[0], list(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # test code\n",
    "# #v0 = [characterDict['CAPTAIN AMERICA'], characterDict['MISS THING/MARY'], characterDict['ORWELL']]\n",
    "# v0 = [characterDict['MISS THING/MARY']]\n",
    "\n",
    "# for i in range(0, len(v0)):\n",
    "#     num_visited_nodes, rdd = sparkBFS(sc, graphRDD, v0[i])\n",
    "    \n",
    "#     print('%s : %d nodes visited' % (vertexDict[str(v0[i])], num_visited_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load vertex dictionary (is basically the character dictionary inverted)\n",
    "reader = csv.reader(open('characters.csv', 'rb'))\n",
    "vertexDict = dict(reader)\n",
    "characterDict = dict(zip(vertexDict.values(), [int(k) for k in vertexDict.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function prepares the rdd\n",
    "def prepare_rdd(filename):\n",
    "    rdd = sc.textFile(filename)\n",
    "\n",
    "    # map string to tuples\n",
    "    rdd = rdd.map(lambda x: x.split(','))\n",
    "    rdd = rdd.map(lambda x: (int(x[0]), int(x[1])))\n",
    "    \n",
    "    # now group s.t. we have for each vertex an adjacency list of nodes\n",
    "    rdd = rdd.groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test code\n",
    "filename = 'edge_list.csv'#'edge_list_simple.csv' # 'edge_list.csv'\n",
    "\n",
    "rdd = prepare_rdd(filename)\n",
    "#rdd.take(5)\n",
    "v0 = [characterDict['CAPTAIN AMERICA'], characterDict['MISS THING/MARY'], characterDict['ORWELL']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTAIN AMERICA : 6407 nodes visited\n",
      "MISS THING/MARY : 6 nodes visited\n",
      "ORWELL : 8 nodes visited\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(v0)):\n",
    "    rdd = prepare_rdd(filename)\n",
    "    num_visited_nodes, rdd = sparkBFS(sc, rdd, v0[i])\n",
    "    \n",
    "    print('%s : %d nodes visited' % (vertexDict[str(v0[i])], num_visited_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_visited_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15', '1621', '3430']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
