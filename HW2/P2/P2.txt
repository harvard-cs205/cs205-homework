In P2_tranfer_times.png, I compare the time taken by transfering memory for fine-grained, medium-grained, coarse-grained locks, as well as for the serial implementation. We investigated the time taken for two cases: [1] when memory is tranferred between random locations, and [2] when memory is transferred to closeby locations (correlated). In both cases, we can see that the green line is much lower than any of the black or red points/lines, indicating that the serial version performs much faster. This is likely because the parallelized computation is a very simple and quick one that consists of merely incrementing and decrementing two variables; the overhead communication costs dominate. Focusing our attention to within the threaded-versions (black and red), first we notice that the black and red overlap (at x = 1), indicating internal consistency; the medium-grained locks should perform essentially the same as the fine-grained lock when N = 1.  Furthermore we can see an overall trend in which larger lock sizes (fewer locks) tend to increase the time taken for both correlated and uncorrelated cases. This may occur because as we increase the lock size, there are more collisions between threads, causing more wait time. When there is a lock for every variable, there will be very few collisions, allowing for less wait time for each thread. The correlated memory transfer case (solid black) interestingly shows there to be a sweet spot near about N = 50~70. This improvement occurs because when the memory transfer is local within the same lock, we save time by activating only one lock instead of two, as usually we need two locks, one for the source location and one for the destination location. 
