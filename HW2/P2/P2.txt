Please see the images (NvsTime_1to30.png and NvsTime_1to60.png) in this directory, which indicate the relationships between N and the time it took. There are two images because one goes from 1 to 30 in steps of 3, and other goes 0 to 60 in steps of 6 to see a bigger set of values.

Uncorrelated testing:
In the uncorrelated data, it seems like N=10 is the best. It makes sense that larger N lead to a slow down, since it would mean threads are often waiting on eachother. If N is too big, then threads will wait more often. If they are too small, you will have more locking than needed.

Correlated testing:
If N < 20, this does quite badly. This makes sense, because in the correlated model, items being switched are within 10 of eachother. So, if N < 20, then we would expect pretty often that multiple locks are needed. If N > 20, it gets better, and for higher values of N, it seems to get better and better until it flattens out around 25. This makes sense because at N=20, you will need two locks only if the lower item is in the last half of the lock group. As N gets bigger, you are less and less likely to need two locks. If N is very large (say 100), then it is again slow since threads are often waiting.