Results:
==========

1 Thread: 2.76906299591 seconds for 10 filter passes.
2 Threads: 1.39691114426 seconds for 10 filter passes.
4 Threads: .727634906769 seconds for 10 filter passes.

Discussion:
=============
Considering the results above, we see, perhaps, what we would naively expect. Two threads gives essentially double performance (+- some noise) and 4 threads gives essentially quadruple performance (+- some noise). It is worth noting that this is true on my quad-core CPU, although when I ran it on my dual-core Mac computer with hyperthreading, 4 threads did not give nearly as large of a performance boost as seen here. This trend was consistent with the other problems, as well. I do not understand hyperthreading sufficiently to discuss why this may be the case, but the four-core CPU is quite simple to understand. There is no locking as data is read-only and we do not need to worry about interfering with shared memory, so more threads essentially equals more work (linearly).

I chose to use events to control cooperation between threads. This seemed to make the most sense given what is described in the problem statement (and what can be arrive at with some thought): thread n on iteration i requires threads n, n-1 and n+1 to have finished iteration i-1. This suggested to me an organizational system in which we store a (num_threads) x (num_iterations) matrix, where the (t, i) entry corresponds to thread t's work on iteration i. If thread t has completed iteration i, event (t, i) is set. If not, event (t, i) is unset. Then threads can simply wait for the corresponding events to be completed before finishing.

The only downfall with such an approach is that we store all of these events long after they are needed. For example, if thread t is working on iteration 10, clearly thread t, t-1, and t+1 have all completed their work on iterations 0-9. For this reason, it's unnecessary to store the fact that these events have been completed. In this case, it was irrelevant: with 4 threads maximum and 10 iterations, a 40 element matrix is hardly anything to worry about. This will be true even increasing the number of iterations by a few orders of magnitude, depending onthe computer the code is being run on. However, in a sort of "massively parallel" case with many threads, or if we want to run many iterations, it would make signficantly more sense to reuse events. One such scheme would be to simply store a smaller matrix and wrap around to the beginning, setting/unsetting events as necessary. Even storing a (n_threads) x (5) (5 chosen arbitrarily) matrix would massively improve memory efficiency, while making it relatively easy to reset events - when the matrix is fully set, we can with absolutely no harm go back and unset the earlier elements, noting that the 0'th entry would require the 4'th entry to be completed when we begin reusing locks (we have to wrap around). Alternatively, there is probably a way to have one event per thread, and have them set/unset that event at the right time as needed. This is more complicated, as now we need to ensure that we do not unset a given lock before another thread that neets that event set has had a chance to check it (this problem is solved by using some constant size matrix - e.g. the 5 chosen above). I deemed that this was an unnecessary level of complexity at this stage and thus did not investigate further. As we discussed in class: there is no need for over-optimization.
