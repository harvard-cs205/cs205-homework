The table below shows performance for each of the stages of implementation, as well as referencing the graph for the relevant sub-question performance.


Implementation		Ave. SFPS		Graph
___________________________________________________________________________

Original			12.7			P5_original.png

Multi-threading
1 thead				12.2			P5_sub1_1thread.png
4 threads			18.1 			P5_sub1_4threads.png

Spatial decomposition
1 thread 			656.5			P5_sub2_1thread.png
4 threads			920.5			P5_sub2_4threads.png

Spatially coherent sorting
1 thread 			876.9 			P5_sub3_1thread.png			
4 threads 			1230.3			P5_sub3_4threads.png

Locking
1 thread 			842.4			P5_sub4_1thread.png			
4 threads 			1091.5			P5_sub4_4threads.png			

SFPS = Simulation frames per second.

Multi-threading:

Performance with 1 thread is roughly in line with the original serial code. Increasing the number of threads from 4 to 1 results in ~1.5x the number of simulation frames per second. The improvement is limited compared to the increase in the number of threads, as a significant portion of the code is memory-bound.

Spatial decomposition:

Enabling the grid resulted in a ~50x increase in the number of simulation frames per second for both the single-threaded and the 4-threaded implementation. Even though there is a slight overhead associated with updating the grid at each iteration, this is vastly offset by the improvement in performance from using the grid in the sub_update function. The speed-up is a result of changing sub_update from a O(N^2) algorithm to a O(N) algorithm, which will have an especially significant effect for large N (in this case, N=10,000).

Spatially coherent sorting:

I used Morton ordering for spatially coherent sorting. This mapping converts (x, y) coordinates into binary representations that are spatially coherent once sorted. As the function is intended to map integers rather than floating-point numbers, I converted the floating-point (x, y) coordinates in integer (x, y) coordinates, using the same approach as the grid mapping i.e. coordinate / grid_spacing.

There is some overhead associated with this additional step (~0.16 seconds in total to map the coordinates, sort positions and velocities, and update the grid), but this is partly offset by the speed-up in the update function. In particular, this implementation benefits from the fact that the balls whose coordinates are close to each other are nearby in memory, meaning that comparisons between ball positions (e.g. when checking for collisions) are less expensive.

More specifically, the one-threaded and four-threaded implementations both see ~1.3x more simulation frames per second after implementing the sorting.

Locking:

Adding locks slows down both the single-threaded and four-threaded implementations. This is not surprising: as the operations are largely bound by bandwidth, any degree of locking will inevitably slow them down. After adding locks, the one-threaded version is completing ~0.96x the number of simulation frames per second, while the four-threaded version is completing ~0.89x the number of simulation frames per second. The slowdown in the one-threaded code is mainly due to the overhead associated with acquiring and releasing locks, while the greater slowdown in the four-threaded version is a result of the locks limiting the performance gains that can be obtained through parallelism.
